# Design

A few notes on some opinionated design decisions present in aitextgen.

## Features

- TokenDataset caching is done via [MessagePack](https://msgpack.org/index.html). Tokens are encoded as integers, which is the case MessagePack is especially designed for, compressing the final file by about 50%. MessagePack will also save/reload the file much faster than `pickle` or `json`.
  - By default, this is additionally compressed via `gzip`, further reducing file size. This may have memory constraints for superlarge files (1GB+), hence a parameter to disable compression.
- Although GPT-2 is the default Transformers model architecture used, there is limited GPT-2 specific code. This allows this tool to easily adapt to new CLM architectures that may be released in the future, such as SparseTransformers or Reformers.
- `generate_to_file()` automatically assigns the generated process a seed if one is not specified. This allows other users to reproduce a generation deterministically (e.g. in a Jupyter Notebook), in order to provide proof that at text was generated by AI and not altered.
- For new Tokenizers, the default end-of-speech token is `<<<end>>>` instead of `<|endoftext|>` as with GPT-2. This is done for two reasons: the `<|endoftext|>` token requires escaping if attempting to parse via regex since `|` is a regex-reserved character (including most bracket characters typically used for special tokens), and because the `<|` and `|>` are special ligature sequences which look funny on fonts which support ligatures, such as [Fira Code](https://github.com/tonsky/FiraCode). (`<<<` and `>>>` are ligature sequences too but they look visually distinct from other delimiter sequences)
  - `<<` and `>>` might be more efficient token wrappers: however they conflict with bitwise operators.

## Philosophies

- The development intent of aitextgen is as a _tool_ for AI text generation, and not a philosophical experiment behind AI consciousness or whatnot. (alternatively, one could argue that _humans_ are the ones who perform actions based on prior knowledge and [free will is a myth](https://www.youtube.com/watch?v=kQjb-EP2JEE), but that's a discussion for another time)

## Deviations from Huggingface Transformers

- While aitextgen was in development, Huggingface added a finetuning `Trainer` to Transformers, which would make this package somewhat redundant. However, pytorch-lightning has a number of specific features and optimizations that make separating out the logic more efficient and easier to modify/tweak.
- The default for `cache_dir`, where the model is stored, (normally a temp folder) is the current working directory to both allow manual introspection, manipulation, and persistence.
- For generation, random sampling is the default while in the base Transformers, [greedy sampling is used](https://github.com/huggingface/transformers/pull/3298) which makes the output more deterministic.

## Why aitextgen was made vs. improving gpt-2-simple

gpt-2-simple was an quick project intended to be able to use GPT-2 in an app when no user-friendly alternative existed at the time, and for others to be able to use GPT-2 without fiddiling around with the command line. However, since development on GPT-2 from OpenAI abruptly and unexpectedly stopped, it would take too much time to manually implement necessary improvements on my end.

It is faster to build a new project on a stronger, more futureproof base than it is to iterate on gpt-2-simple. Both transformers and pytorch-lightning are in rapid development so I am confident they will be around ffor awhile.

I am also trying to avoid feature creep, as that happened with textgenrnn and gpt-2-simple and made the packages more difficult to maintain. Not all Issues/PRs will be added.

## Why create a separate package instead of commiting directly to Transformers

aitextgen has specific optimizations that may be out of scope for the Transformers package primary use cases. If an optimization benefits the default use cases of Transformers without negatively impacting it, I may do a pull request.

As the license of this repo is permissible (MIT), the Huggingface team is able to apply any modifications/ideas to their own repo if necessary without explicit permission.
