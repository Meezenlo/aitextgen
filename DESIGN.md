# Design

A few notes on some opinionated design decisions present in aitextgen.

## Features

- TokenDataset caching is done via [MessagePack](https://msgpack.org/index.html). Tokens are encoded as integers, which is the case MessagePack is especially designed for, compressing the final file by about 50%. MessagePack will also save/reload the file much faster than `pickle` or `json`.
  - By default, this is additionally compressed via `gzip`, further reducing file size. This may have memory constraints for superlarge files (1GB+), hence a parameter to disable compression.
- Although GPT-2 is the default Transformers model architecture used, there is limited GPT-2 specific code. This allows this tool to easily adapt to new CLM architectures that may be released in the future, such as SparseTransformers or Reformers.
- `generate_to_file()` automatically assigns the generated process a seed if one is not specified. This allows other users to reproduce a generation deterministically (e.g. in a Jupyter Notebook), in order to provide proof that at text was generated by AI and not altered.
- For new Tokenizers, the default end-of-speech token is `<<<end>>>` instead of `<|endoftext|>` as with GPT-2. This is done for two reasons: the `<|endoftext|>` token requires escaping if attempting to parse via regex since `|` is a regex-reserved character (including most bracket characters typically used for special tokens), and because the `<|` and `|>` are special ligature sequences which look funny on fonts which support ligatures. (`<<<` and `>>>` are ligature sequences too but they look visually distinct from other delimiter sequences)
  - `<<` and `>>` might be more efficient token wrappers: however they conflict with bitwise operators.

## Philosophies

- The development intent of aitextgen is as a _tool_ for AI text generation, and not a philosophical experiment behind AI consciousness or whatnot. (alternatively, one could argue that _humans_ are the ones who perform actions based on prior knowledge and [free will is a myth](https://www.youtube.com/watch?v=kQjb-EP2JEE), but that's a discussion for another time)

## Deviations from Huggingface Transformers

- The default for `cache_dir`, where the model is stored, (normally a temp folder) is the current working directory to both allow manual introspection, manipulation, and persistence.
- Several features/logging messages were removed, as they are only applicable for a very small minority of potential users of this particular package (e.g. training validation and beam generation). I am fine with readding such things if needed, but I am trying to avoid complexity creep, as that happened with textgenrnn and gpt-2-simple and made the packages more difficult to maintain.
- For generation, random sampling is the default while in the base Transformers, [greedy sampling is used](https://github.com/huggingface/transformers/pull/3298) which makes the output more deterministic.

## Why aitextgen was made vs. improving gpt-2-simple

gpt-2-simple was an quick project intended to be able to use GPT-2 in an app when no good alternative existed at the time, and for others to be able to use GPT-2 without fiddiling around with random code. However, since development on GPT-2 abruptly and unexpeectedly stopped, it would take too much time to manually implement necessary improvements on my end.

It is faster to build a new project on a stronger, more futureproof base than it is to iterate on gpt-2-simple.

## Why create a separate package instead of commiting directly to Transformers

aitextgen has specific optimizations that may be out of scope for the Transformers use cases. If an optimization benefits the default uses cases of Transformers without negatively impacting it, I may do a pull request.

As the license of this repo is permissible (MIT), the Huggingface team is able to apply any modifications/ideas to their own repo if necessary without explicit permission.
